{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-31-c8a9effe7464>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-31-c8a9effe7464>\"\u001b[1;36m, line \u001b[1;32m32\u001b[0m\n\u001b[1;33m    t1[i] = t1[i].lower()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# author:tony.qin\n",
    "# subject: Purdue homework2-1 _ problem1 TF-IDF\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import nltk.tokenize as tk\n",
    "from helper import remove_punc\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from helper import remove_punc\n",
    "\n",
    "#Clean and stem the contents of a document\n",
    "#Takes in a file name to read in and clean\n",
    "#Return a list of words, without stopwords and punctuation, and with all words stemmed\n",
    "# NOTE: Do not append any directory names to doc -- assume we will give you\n",
    "# a string representing a file name that will open correctly\n",
    "def readAndCleanDoc(doc) :\n",
    "    #1. Open document, read text into *single* string\n",
    "    str = open(doc, 'r').read()\n",
    "    \n",
    "    #2. Tokenize string using nltk.tokenize.word_tokenize\n",
    "    t = nltk.tokenize.word_tokenize(str)\n",
    "    \n",
    "    #3. Filter out punctuation from list of words (use remove_punc)\n",
    "    t1 = remove_punc(t)\n",
    "    \n",
    "    #4. Make the words lower case\n",
    "    for i in range(len(t1)): \n",
    "    t1[i] = t1[i].lower()\n",
    "    \n",
    "    #5. Filter out stopwords\n",
    "    t2_without_sw = [word for word in t1 if not word in stopwords.words()]\n",
    "\n",
    "    #6. Stem words\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemed_words = [ps(word) for word in t2_without_sw]\n",
    "    \n",
    "    #7: Return stemmed words\n",
    "    return stemed_words\n",
    "    \n",
    "#Builds a doc-word matrix for a set of documents\n",
    "#Takes in a *list of filenames*\n",
    "#\n",
    "#Returns 1) a doc-word matrix for the cleaned documents\n",
    "#This should be a 2-dimensional numpy array, with one row per document and one \n",
    "#column per word (there should be as many columns as unique words that appear\n",
    "#across *all* documents. Also, Before constructing the doc-word matrix, \n",
    "#you should sort the wordlist output and construct the doc-word matrix based on the sorted list\n",
    "#\n",
    "#Also returns 2) a list of words that should correspond to the columns in\n",
    "#docword\n",
    "def buildDocWordMatrix(doclist) :\n",
    "    docword = []\n",
    "    #1. Create word lists for each cleaned doc (use readAndCleanDoc)\n",
    "    for i in range(len(doclist)):\n",
    "        docword[i] = readAndCleanDoc(doclist[i])\n",
    "        docword.append()\n",
    "    \n",
    "    #2. Use these word lists to build the doc word matrix\n",
    "    wordlist = list(docword)\n",
    "            \n",
    "    return docword, wordlist\n",
    "    \n",
    "#Builds a term-frequency matrix\n",
    "#Takes in a doc word matrix (as built in buildDocWordMatrix)\n",
    "#Returns a term-frequency matrix, which should be a 2-dimensional numpy array\n",
    "#with the same shape as docword\n",
    "def buildTFMatrix(docword) :\n",
    "    #fill in\n",
    "    tf = {}\n",
    "    for word,count in docword.items():\n",
    "        tf{word} = count / len(docword)\n",
    "\n",
    "    return tf\n",
    "    \n",
    "#Builds an inverse document frequency matrix\n",
    "#Takes in a doc word matrix (as built in buildDocWordMatrix)\n",
    "#Returns an inverse document frequency matrix (should be a 1xW numpy array where\n",
    "#W is the number of words in the doc word matrix)\n",
    "#Don't forget the log factor!\n",
    "def buildIDFMatrix(docword) :\n",
    "    #fill in\n",
    "    import math\n",
    "    idf = {}\n",
    "    N = len(docword)\n",
    "    \n",
    "    idf = dict.fromkeys(docword[0].keys(),0)\n",
    "    for doc in docword:\n",
    "        for word,val in doc.items():\n",
    "            if val > 0:\n",
    "                idf[word] += 1\n",
    "                \n",
    "    for word,val in idf.items():\n",
    "        idf[word] = math.loc10(N / float(val))\n",
    "    \n",
    "    \n",
    "    return idf\n",
    "    \n",
    "#Builds a tf-idf matrix given a doc word matrix\n",
    "def buildTFIDFMatrix(docword) :\n",
    "    #fill in\n",
    "    for i in docword.items():\n",
    "        tfidf[i] = tf[i]*idf[i]\n",
    "\n",
    "    return tfidf\n",
    "    \n",
    "#Find the three most distinctive words, according to TFIDF, in each document\n",
    "#Input: a docword matrix, a wordlist (corresponding to columns) and a doclist \n",
    "# (corresponding to rows)\n",
    "#Output: a dictionary, mapping each document name from doclist to an (ordered\n",
    "# list of the three most common words in each document\n",
    "def findDistinctiveWords(docword, wordlist, doclist) :\n",
    "    distinctiveWords = {}\n",
    "    \n",
    "    distinctiveWords{} = np.argsort(docword[], axis=0) \n",
    "    \n",
    "    #you might find numpy.argsort helpful for solving this problem:\n",
    "    #https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n",
    "    \n",
    "    \n",
    "    return distinctiveWords\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join, splitext\n",
    "    \n",
    "    ### Test Cases ###\n",
    "    directory='lecs'\n",
    "    path1 = join(directory, '1_vidText.txt')\n",
    "    path2 = join(directory, '2_vidText.txt')\n",
    "    \n",
    "    # Uncomment and recomment ths part where you see fit for testing purposes\n",
    "    '''\n",
    "    print(\"*** Testing readAndCleanDoc ***\")\n",
    "    print(readAndCleanDoc(path1)[0:5])\n",
    "    print(\"*** Testing buildDocWordMatrix ***\") \n",
    "    doclist =[path1, path2]\n",
    "    docword, wordlist = buildDocWordMatrix(doclist)\n",
    "    print(docword.shape)\n",
    "    print(len(wordlist))\n",
    "    print(docword[0][0:10])\n",
    "    print(wordlist[0:10])\n",
    "    print(docword[1][0:10])\n",
    "    print(\"*** Testing buildTFMatrix ***\") \n",
    "    tf = buildTFMatrix(docword)\n",
    "    print(tf[0][0:10])\n",
    "    print(tf[1][0:10])\n",
    "    print(tf.sum(axis =1))\n",
    "    print(\"*** Testing buildIDFMatrix ***\") \n",
    "    idf = buildIDFMatrix(docword)\n",
    "    print(idf[0][0:10])\n",
    "    print(\"*** Testing buildTFIDFMatrix ***\") \n",
    "    tfidf = buildTFIDFMatrix(docword)\n",
    "    print(tfidf.shape)\n",
    "    print(tfidf[0][0:10])\n",
    "    print(tfidf[1][0:10])\n",
    "    print(\"*** Testing findDistinctiveWords ***\")\n",
    "    print(findDistinctiveWords(docword, wordlist, doclist))\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
